{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ede5d3",
   "metadata": {},
   "source": [
    "### **KantaiBERT**  is trained as a `RoBERTa` Transformer with `DistilBERT` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the dataset.\n",
    "!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output \"kant.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83956a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:24.133215Z",
     "iopub.status.busy": "2023-02-08T09:03:24.132128Z",
     "iopub.status.idle": "2023-02-08T09:03:26.745683Z",
     "shell.execute_reply": "2023-02-08T09:03:26.744484Z",
     "shell.execute_reply.started": "2023-02-08T09:03:24.133121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers                            0.12.1\n",
      "transformers                          4.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710306b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:26.753704Z",
     "iopub.status.busy": "2023-02-08T09:03:26.752729Z",
     "iopub.status.idle": "2023-02-08T09:03:26.775669Z",
     "shell.execute_reply": "2023-02-08T09:03:26.774640Z",
     "shell.execute_reply.started": "2023-02-08T09:03:26.753663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kant.txt',\n",
       " 'KantaiBERT/merges.txt',\n",
       " 'wandb/run-20230208_085425-26qasbzp/files/requirements.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path('.').glob('**/*.txt')]\n",
    "\n",
    "#Initialize a tokenizer.\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b8c0bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:26.777473Z",
     "iopub.status.busy": "2023-02-08T09:03:26.777041Z",
     "iopub.status.idle": "2023-02-08T09:03:31.404525Z",
     "shell.execute_reply": "2023-02-08T09:03:31.403472Z",
     "shell.execute_reply.started": "2023-02-08T09:03:26.777420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Customize training.\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d8886d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:31.408092Z",
     "iopub.status.busy": "2023-02-08T09:03:31.407766Z",
     "iopub.status.idle": "2023-02-08T09:03:31.430130Z",
     "shell.execute_reply": "2023-02-08T09:03:31.428876Z",
     "shell.execute_reply.started": "2023-02-08T09:03:31.408063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save files to disk.\n",
    "import os\n",
    "token_dir = 'KantaiBERT'\n",
    "\n",
    "if not os.path.exists(token_dir):\n",
    "    os.makedirs(token_dir)\n",
    "    \n",
    "tokenizer.save_model('KantaiBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2691e2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:31.432629Z",
     "iopub.status.busy": "2023-02-08T09:03:31.431900Z",
     "iopub.status.idle": "2023-02-08T09:03:31.487941Z",
     "shell.execute_reply": "2023-02-08T09:03:31.486846Z",
     "shell.execute_reply.started": "2023-02-08T09:03:31.432589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ä Critique', 'Ä of', 'Ä Pure', 'Ä Reason']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the trained tokenizer files.\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    'KantaiBERT/vocab.json', \n",
    "    'KantaiBERT/merges.txt')\n",
    "\n",
    "\n",
    "tokenizer.encode('The Critique of Pure Reason').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26bf51ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:31.490194Z",
     "iopub.status.busy": "2023-02-08T09:03:31.489791Z",
     "iopub.status.idle": "2023-02-08T09:03:31.500216Z",
     "shell.execute_reply": "2023-02-08T09:03:31.499094Z",
     "shell.execute_reply.started": "2023-02-08T09:03:31.490156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('The Critique of Pure Reason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ba8769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:31.502664Z",
     "iopub.status.busy": "2023-02-08T09:03:31.502160Z",
     "iopub.status.idle": "2023-02-08T09:03:31.510461Z",
     "shell.execute_reply": "2023-02-08T09:03:31.509452Z",
     "shell.execute_reply.started": "2023-02-08T09:03:31.502628Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    ('', tokenizer.token_to_id('')), \n",
    "    ('', tokenizer.token_to_id(''))\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be78be72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:31.512629Z",
     "iopub.status.busy": "2023-02-08T09:03:31.512084Z",
     "iopub.status.idle": "2023-02-08T09:03:31.695689Z",
     "shell.execute_reply": "2023-02-08T09:03:31.693864Z",
     "shell.execute_reply.started": "2023-02-08T09:03:31.512593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define model config.\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(vocab_size = 52_000, \n",
    "                       max_position_embeddings = 514,\n",
    "                       num_attention_heads = 12, \n",
    "                       num_hidden_layers = 6,\n",
    "                       type_vocab_size = 1\n",
    "                      )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d9347c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:31.697661Z",
     "iopub.status.busy": "2023-02-08T09:03:31.697255Z",
     "iopub.status.idle": "2023-02-08T09:03:35.288377Z",
     "shell.execute_reply": "2023-02-08T09:03:35.287257Z",
     "shell.execute_reply.started": "2023-02-08T09:03:31.697622Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#Re-create the tokenizer in Transformers.\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./KantaiBERT', \n",
    "                                             max_length = 512)\n",
    "\n",
    "#Initialize a model from scratch.\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bbe8f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:35.291298Z",
     "iopub.status.busy": "2023-02-08T09:03:35.290177Z",
     "iopub.status.idle": "2023-02-08T09:03:35.298827Z",
     "shell.execute_reply": "2023-02-08T09:03:35.297891Z",
     "shell.execute_reply.started": "2023-02-08T09:03:35.291247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83504416\n"
     ]
    }
   ],
   "source": [
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959b7bca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:03:35.300717Z",
     "iopub.status.busy": "2023-02-08T09:03:35.300184Z",
     "iopub.status.idle": "2023-02-08T09:04:05.974248Z",
     "shell.execute_reply": "2023-02-08T09:04:05.973181Z",
     "shell.execute_reply.started": "2023-02-08T09:03:35.300676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#Build the dataset.\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(tokenizer = tokenizer, \n",
    "                                file_path = './kant.txt',\n",
    "                                block_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a26c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:04:15.018623Z",
     "iopub.status.busy": "2023-02-08T09:04:15.017583Z",
     "iopub.status.idle": "2023-02-08T09:04:15.024551Z",
     "shell.execute_reply": "2023-02-08T09:04:15.023476Z",
     "shell.execute_reply.started": "2023-02-08T09:04:15.018582Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define a data collator.\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, \n",
    "                                                mlm = True, \n",
    "                                                mlm_probability = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "975b15ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:04:16.787956Z",
     "iopub.status.busy": "2023-02-08T09:04:16.787183Z",
     "iopub.status.idle": "2023-02-08T09:04:19.448488Z",
     "shell.execute_reply": "2023-02-08T09:04:19.447482Z",
     "shell.execute_reply.started": "2023-02-08T09:04:16.787915Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize the trainer.\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './KantaiBERT/',\n",
    "    overwrite_output_dir = True,\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 128, \n",
    "    save_steps = 10_000,\n",
    "    save_total_limit = 2)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args, \n",
    "    data_collator = data_collator,\n",
    "    train_dataset = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eacf9ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:04:24.033779Z",
     "iopub.status.busy": "2023-02-08T09:04:24.033084Z",
     "iopub.status.idle": "2023-02-08T09:31:54.758876Z",
     "shell.execute_reply": "2023-02-08T09:31:54.757914Z",
     "shell.execute_reply.started": "2023-02-08T09:04:24.033740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 170964\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6680\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimata\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230208_090424-2rs58ahb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kimata/huggingface/runs/2rs58ahb\" target=\"_blank\">./KantaiBERT/</a></strong> to <a href=\"https://wandb.ai/kimata/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6680' max='6680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6680/6680 27:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.567300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 7s, sys: 5.9 s, total: 27min 13s\n",
      "Wall time: 27min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6680, training_loss=4.201414690760082, metrics={'train_runtime': 1650.6877, 'train_samples_per_second': 517.857, 'train_steps_per_second': 4.047, 'total_flos': 4507986008547840.0, 'train_loss': 4.201414690760082, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c5c427",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:31:54.765820Z",
     "iopub.status.busy": "2023-02-08T09:31:54.761185Z",
     "iopub.status.idle": "2023-02-08T09:31:57.838335Z",
     "shell.execute_reply": "2023-02-08T09:31:57.836688Z",
     "shell.execute_reply.started": "2023-02-08T09:31:54.765771Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./KantaiBERT\n",
      "Configuration saved in ./KantaiBERT/config.json\n",
      "Model weights saved in ./KantaiBERT/pytorch_model.bin\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "2023-02-08 09:31:56.212336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.213187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.213650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.214406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 09:31:56.214698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.215127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.215515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.216095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.216527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.216935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-08 09:31:56.217279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2817 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file ./KantaiBERT/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./KantaiBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Didn't find file ./KantaiBERT/tokenizer.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/added_tokens.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/tokenizer_config.json. We won't load it.\n",
      "loading file ./KantaiBERT/vocab.json\n",
      "loading file ./KantaiBERT/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Adding <pad> to the vocabulary\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Save the model.\n",
    "trainer.save_model('./KantaiBERT')\n",
    "\n",
    "#Language modeling with FillMaskPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    'fill-mask',\n",
    "    model = './KantaiBERT',\n",
    "    tokenizer = './KantaiBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1dd55da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:33:02.064314Z",
     "iopub.status.busy": "2023-02-08T09:33:02.063913Z",
     "iopub.status.idle": "2023-02-08T09:33:02.174982Z",
     "shell.execute_reply": "2023-02-08T09:33:02.173993Z",
     "shell.execute_reply.started": "2023-02-08T09:33:02.064273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2920242249965668,\n",
       "  'token': 266,\n",
       "  'token_str': ' of',\n",
       "  'sequence': 'Human thinking of reason.'},\n",
       " {'score': 0.15277273952960968,\n",
       "  'token': 610,\n",
       "  'token_str': ' practical',\n",
       "  'sequence': 'Human thinking practical reason.'},\n",
       " {'score': 0.06844190508127213,\n",
       "  'token': 465,\n",
       "  'token_str': ' pure',\n",
       "  'sequence': 'Human thinking pure reason.'},\n",
       " {'score': 0.059538062661886215,\n",
       "  'token': 1005,\n",
       "  'token_str': ' speculative',\n",
       "  'sequence': 'Human thinking speculative reason.'},\n",
       " {'score': 0.03545363247394562,\n",
       "  'token': 12,\n",
       "  'token_str': ',',\n",
       "  'sequence': 'Human thinking, reason.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('Human thinking <mask> reason.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1890a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:34:23.653453Z",
     "iopub.status.busy": "2023-02-08T09:34:23.652366Z",
     "iopub.status.idle": "2023-02-08T09:34:24.685820Z",
     "shell.execute_reply": "2023-02-08T09:34:24.684727Z",
     "shell.execute_reply.started": "2023-02-08T09:34:23.653383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "The Project Gutenberg EBook of The Critique of Pure Reason, by Immanuel Kant\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "\n",
      "Title: The Critique of Pure Reason\n"
     ]
    }
   ],
   "source": [
    "!head -10 /kaggle/working/kant.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89594b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:34:50.442633Z",
     "iopub.status.busy": "2023-02-08T09:34:50.442200Z",
     "iopub.status.idle": "2023-02-08T09:34:50.558395Z",
     "shell.execute_reply": "2023-02-08T09:34:50.557481Z",
     "shell.execute_reply.started": "2023-02-08T09:34:50.442594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.02685278281569481,\n",
       "  'token': 1274,\n",
       "  'token_str': ' Reason',\n",
       "  'sequence': 'The Reason of Pure Reason'},\n",
       " {'score': 0.023974640294909477,\n",
       "  'token': 2260,\n",
       "  'token_str': ' Critique',\n",
       "  'sequence': 'The Critique of Pure Reason'},\n",
       " {'score': 0.010829992592334747,\n",
       "  'token': 899,\n",
       "  'token_str': ' faculty',\n",
       "  'sequence': 'The faculty of Pure Reason'},\n",
       " {'score': 0.009400766342878342,\n",
       "  'token': 3092,\n",
       "  'token_str': ' Conceptions',\n",
       "  'sequence': 'The Conceptions of Pure Reason'},\n",
       " {'score': 0.009031211026012897,\n",
       "  'token': 415,\n",
       "  'token_str': ' conception',\n",
       "  'sequence': 'The conception of Pure Reason'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('The <mask> of Pure Reason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15f875e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T09:35:20.127215Z",
     "iopub.status.busy": "2023-02-08T09:35:20.126840Z",
     "iopub.status.idle": "2023-02-08T09:35:20.238416Z",
     "shell.execute_reply": "2023-02-08T09:35:20.237497Z",
     "shell.execute_reply.started": "2023-02-08T09:35:20.127184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.31438520550727844,\n",
       "  'token': 1274,\n",
       "  'token_str': ' Reason',\n",
       "  'sequence': 'The Critique of Pure Reason'},\n",
       " {'score': 0.07076602429151535,\n",
       "  'token': 14,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'The Critique of Pure.'},\n",
       " {'score': 0.0475216805934906,\n",
       "  'token': 1423,\n",
       "  'token_str': ' Pure',\n",
       "  'sequence': 'The Critique of Pure Pure'},\n",
       " {'score': 0.029480906203389168,\n",
       "  'token': 2985,\n",
       "  'token_str': ' Practical',\n",
       "  'sequence': 'The Critique of Pure Practical'},\n",
       " {'score': 0.018943089991807938,\n",
       "  'token': 2006,\n",
       "  'token_str': ' Transcendental',\n",
       "  'sequence': 'The Critique of Pure Transcendental'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('The Critique of Pure <mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6344e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
